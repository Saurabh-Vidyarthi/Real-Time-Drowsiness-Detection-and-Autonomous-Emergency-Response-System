{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e8f035a",
   "metadata": {},
   "source": [
    "## Creating The DataSet for training Awake or drowsy labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e8b5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# Create a folder to store the captured images\n",
    "output_folder = 'aw_2'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Initialize the RealSense camera capture\n",
    "# Replace '0' with the appropriate index for the RealSense camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Check if the camera is opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Unable to open RealSense camera.\")\n",
    "else:\n",
    "    count = 0  # Counter for captured images\n",
    "    while True:\n",
    "        if(count==15):\n",
    "            break\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Display the frame\n",
    "        cv2.imshow('RealSense Camera Feed', frame)\n",
    "\n",
    "        # Check for the Enter key press to capture an image\n",
    "        if cv2.waitKey(1) == 13:  # 13 is the ASCII code for Enter key\n",
    "            # Save the captured image\n",
    "            img_name = f'image_{count}.png'\n",
    "            img_path = os.path.join(output_folder, img_name)\n",
    "            cv2.imwrite(img_path, frame)\n",
    "            print(f\"Saved {img_name}\")\n",
    "            count += 1\n",
    "\n",
    "        # Check for the 'q' key press to quit\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Release the camera capture\n",
    "cap.release()\n",
    "\n",
    "# Close all OpenCV windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b6dbe3",
   "metadata": {},
   "source": [
    "## MODEL TRAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481e724a",
   "metadata": {},
   "source": [
    "### Pytorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "992a6814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25, Loss: 1.1863\n",
      "Epoch 2/25, Loss: 0.8162\n",
      "Epoch 3/25, Loss: 0.5794\n",
      "Epoch 4/25, Loss: 0.4986\n",
      "Epoch 5/25, Loss: 0.4359\n",
      "Epoch 6/25, Loss: 0.3219\n",
      "Epoch 7/25, Loss: 0.3539\n",
      "Epoch 8/25, Loss: 0.2378\n",
      "Epoch 9/25, Loss: 0.2248\n",
      "Epoch 10/25, Loss: 0.1667\n",
      "Epoch 11/25, Loss: 0.1676\n",
      "Epoch 12/25, Loss: 0.1674\n",
      "Epoch 13/25, Loss: 0.1418\n",
      "Epoch 14/25, Loss: 0.1090\n",
      "Epoch 15/25, Loss: 0.1312\n",
      "Epoch 16/25, Loss: 0.1550\n",
      "Epoch 17/25, Loss: 0.0955\n",
      "Epoch 18/25, Loss: 0.1339\n",
      "Epoch 19/25, Loss: 0.1120\n",
      "Epoch 20/25, Loss: 0.1228\n",
      "Epoch 21/25, Loss: 0.1123\n",
      "Epoch 22/25, Loss: 0.1923\n",
      "Epoch 23/25, Loss: 0.1880\n",
      "Epoch 24/25, Loss: 0.0814\n",
      "Epoch 25/25, Loss: 0.1513\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet18\n",
    "from torch.nn import Sequential, Linear, ReLU\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose, Resize\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# Define the preprocessing transformation for input frames\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    Resize((224, 224)),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Define a custom dataset class for your data\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = sorted(os.listdir(root_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum(len(files) for _, _, files in os.walk(self.root_dir))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        class_name = self.classes[idx // 30]  # Assuming there are 12 images per class\n",
    "        class_dir = os.path.join(self.root_dir, class_name)\n",
    "        img_idx = idx % 30    ## chnages here\n",
    "        img_name = os.listdir(class_dir)[img_idx]\n",
    "        img_path = os.path.join(class_dir, img_name)\n",
    "        image = cv2.imread(img_path)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = 0 if class_name == 'awake' else 1\n",
    "        return image, label\n",
    "\n",
    "# Define paths to your dataset directories\n",
    "train_data_dir = 'Aw_Dr_Model_Data'\n",
    "\n",
    "# Create custom dataset for training\n",
    "train_dataset = CustomDataset(train_data_dir, transform=preprocess)\n",
    "\n",
    "# Define data loader\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Load the pre-trained ResNet18 model\n",
    "model = Sequential(\n",
    "    resnet18(pretrained=True),\n",
    "    Linear(1000, 2)  # 2 output classes for 'Awake' and 'Drowsy'\n",
    ")\n",
    "\n",
    "# Freeze all layers except the last one\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model[-1].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Set device to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 25\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "899ed82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.weight tensor([[-0.0092, -0.0099, -0.0098,  ...,  0.0252,  0.0246, -0.0271],\n",
      "        [-0.0119,  0.0097, -0.0256,  ...,  0.0084,  0.0241,  0.0098]])\n",
      "1.bias tensor([-0.0151,  0.0308])\n"
     ]
    }
   ],
   "source": [
    "# Print model parameters\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ffc502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path where you want to save the model\n",
    "model_path = \"unquantized_resnet_model_for_awake_dr_pred.pth\"\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), model_path)\n",
    "\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0b5957",
   "metadata": {},
   "source": [
    "## MODEL INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d898428",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet18\n",
    "from torch.nn import Sequential, Linear, ReLU\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469801b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Open a video capture object (0 for webcam)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Check if the camera is opened correctly\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open camera.\")\n",
    "    exit()\n",
    "\n",
    "# Define class labels\n",
    "class_labels = ['Awake', 'Drowsy']\n",
    "\n",
    "# Read frames from the video capture object\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Preprocess the frame\n",
    "    input_tensor = preprocess(frame).unsqueeze(0).to(device)\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        predicted_label = class_labels[predicted.item()]\n",
    "\n",
    "    # Display the predicted label on the frame\n",
    "    cv2.putText(frame, predicted_label, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # Delay for 1 second\n",
    "    time.sleep(0.01)\n",
    "\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce8665b",
   "metadata": {},
   "source": [
    "### Loading the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bed7908f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Public\\Anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from resnet_model_for_awake_dr_pred.pth\n"
     ]
    }
   ],
   "source": [
    "# Define the file path where the model is saved\n",
    "model_path = \"resnet_model_for_awake_dr_pred.pth\"\n",
    "\n",
    "\n",
    "# Define the model architecture\n",
    "model_loaded = Sequential(\n",
    "    resnet18(pretrained=True),\n",
    "    Linear(1000, 2)  # Assuming 2 output classes\n",
    ")\n",
    "\n",
    "# Load the model state dictionary\n",
    "model_loaded.load_state_dict(torch.load(model_path))\n",
    "\n",
    "print(f\"Model loaded from {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a1d8aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       "  )\n",
       "  (1): Linear(in_features=1000, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f50a893d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Parameters: 11691514\n",
      "Memory Footprint (in bytes): 46766056\n",
      "Memory Footprint (in megabytes): 44.599586486816406\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Calculate the number of parameters\n",
    "total_params = sum(p.numel() for p in model_loaded.parameters())\n",
    "\n",
    "# Calculate the memory footprint (assuming single-precision floating-point numbers)\n",
    "memory_footprint = total_params * 4  # 4 bytes per parameter (32 bits)\n",
    "\n",
    "print(\"Total Number of Parameters:\", total_params)\n",
    "print(\"Memory Footprint (in bytes):\", memory_footprint)\n",
    "print(\"Memory Footprint (in megabytes):\", memory_footprint / (1024 * 1024))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "820aba04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter '0.conv1.weight' has data type: torch.float32\n",
      "Parameter '0.bn1.weight' has data type: torch.float32\n",
      "Parameter '0.bn1.bias' has data type: torch.float32\n",
      "Parameter '0.layer1.0.conv1.weight' has data type: torch.float32\n",
      "Parameter '0.layer1.0.bn1.weight' has data type: torch.float32\n",
      "Parameter '0.layer1.0.bn1.bias' has data type: torch.float32\n",
      "Parameter '0.layer1.0.conv2.weight' has data type: torch.float32\n",
      "Parameter '0.layer1.0.bn2.weight' has data type: torch.float32\n",
      "Parameter '0.layer1.0.bn2.bias' has data type: torch.float32\n",
      "Parameter '0.layer1.1.conv1.weight' has data type: torch.float32\n",
      "Parameter '0.layer1.1.bn1.weight' has data type: torch.float32\n",
      "Parameter '0.layer1.1.bn1.bias' has data type: torch.float32\n",
      "Parameter '0.layer1.1.conv2.weight' has data type: torch.float32\n",
      "Parameter '0.layer1.1.bn2.weight' has data type: torch.float32\n",
      "Parameter '0.layer1.1.bn2.bias' has data type: torch.float32\n",
      "Parameter '0.layer2.0.conv1.weight' has data type: torch.float32\n",
      "Parameter '0.layer2.0.bn1.weight' has data type: torch.float32\n",
      "Parameter '0.layer2.0.bn1.bias' has data type: torch.float32\n",
      "Parameter '0.layer2.0.conv2.weight' has data type: torch.float32\n",
      "Parameter '0.layer2.0.bn2.weight' has data type: torch.float32\n",
      "Parameter '0.layer2.0.bn2.bias' has data type: torch.float32\n",
      "Parameter '0.layer2.0.downsample.0.weight' has data type: torch.float32\n",
      "Parameter '0.layer2.0.downsample.1.weight' has data type: torch.float32\n",
      "Parameter '0.layer2.0.downsample.1.bias' has data type: torch.float32\n",
      "Parameter '0.layer2.1.conv1.weight' has data type: torch.float32\n",
      "Parameter '0.layer2.1.bn1.weight' has data type: torch.float32\n",
      "Parameter '0.layer2.1.bn1.bias' has data type: torch.float32\n",
      "Parameter '0.layer2.1.conv2.weight' has data type: torch.float32\n",
      "Parameter '0.layer2.1.bn2.weight' has data type: torch.float32\n",
      "Parameter '0.layer2.1.bn2.bias' has data type: torch.float32\n",
      "Parameter '0.layer3.0.conv1.weight' has data type: torch.float32\n",
      "Parameter '0.layer3.0.bn1.weight' has data type: torch.float32\n",
      "Parameter '0.layer3.0.bn1.bias' has data type: torch.float32\n",
      "Parameter '0.layer3.0.conv2.weight' has data type: torch.float32\n",
      "Parameter '0.layer3.0.bn2.weight' has data type: torch.float32\n",
      "Parameter '0.layer3.0.bn2.bias' has data type: torch.float32\n",
      "Parameter '0.layer3.0.downsample.0.weight' has data type: torch.float32\n",
      "Parameter '0.layer3.0.downsample.1.weight' has data type: torch.float32\n",
      "Parameter '0.layer3.0.downsample.1.bias' has data type: torch.float32\n",
      "Parameter '0.layer3.1.conv1.weight' has data type: torch.float32\n",
      "Parameter '0.layer3.1.bn1.weight' has data type: torch.float32\n",
      "Parameter '0.layer3.1.bn1.bias' has data type: torch.float32\n",
      "Parameter '0.layer3.1.conv2.weight' has data type: torch.float32\n",
      "Parameter '0.layer3.1.bn2.weight' has data type: torch.float32\n",
      "Parameter '0.layer3.1.bn2.bias' has data type: torch.float32\n",
      "Parameter '0.layer4.0.conv1.weight' has data type: torch.float32\n",
      "Parameter '0.layer4.0.bn1.weight' has data type: torch.float32\n",
      "Parameter '0.layer4.0.bn1.bias' has data type: torch.float32\n",
      "Parameter '0.layer4.0.conv2.weight' has data type: torch.float32\n",
      "Parameter '0.layer4.0.bn2.weight' has data type: torch.float32\n",
      "Parameter '0.layer4.0.bn2.bias' has data type: torch.float32\n",
      "Parameter '0.layer4.0.downsample.0.weight' has data type: torch.float32\n",
      "Parameter '0.layer4.0.downsample.1.weight' has data type: torch.float32\n",
      "Parameter '0.layer4.0.downsample.1.bias' has data type: torch.float32\n",
      "Parameter '0.layer4.1.conv1.weight' has data type: torch.float32\n",
      "Parameter '0.layer4.1.bn1.weight' has data type: torch.float32\n",
      "Parameter '0.layer4.1.bn1.bias' has data type: torch.float32\n",
      "Parameter '0.layer4.1.conv2.weight' has data type: torch.float32\n",
      "Parameter '0.layer4.1.bn2.weight' has data type: torch.float32\n",
      "Parameter '0.layer4.1.bn2.bias' has data type: torch.float32\n",
      "Parameter '0.fc.weight' has data type: torch.float32\n",
      "Parameter '0.fc.bias' has data type: torch.float32\n",
      "Parameter '1.weight' has data type: torch.float32\n",
      "Parameter '1.bias' has data type: torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the parameters and print their data types\n",
    "for name, param in model_loaded.named_parameters():\n",
    "    print(f\"Parameter '{name}' has data type: {param.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b8c978b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1cf9aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet18\n",
    "from torch.nn import Sequential, Linear, ReLU\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose, Resize\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "\n",
    "# Define the preprocessing transformation for input frames\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    Resize((224, 224)),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af796a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to evaluation mode\n",
    "model_loaded.eval()\n",
    "\n",
    "# Open a video capture object (0 for webcam)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Check if the camera is opened correctly\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open camera.\")\n",
    "    exit()\n",
    "\n",
    "# Define class labels\n",
    "class_labels = ['Awake', 'Drowsy']\n",
    "\n",
    "# Read frames from the video capture object\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Preprocess the frame\n",
    "    input_tensor = preprocess(frame).unsqueeze(0)\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        output = model_loaded(input_tensor)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        predicted_label = class_labels[predicted.item()]\n",
    "\n",
    "    # Display the predicted label on the frame\n",
    "    cv2.putText(frame, predicted_label, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # Delay for 1 second\n",
    "    time.sleep(0.01)\n",
    "\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb77c29d",
   "metadata": {},
   "source": [
    "### Inference with the emergency sound on drowsy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6060c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pygame\n",
    "import os\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# Initialize Pygame for audio playback\n",
    "pygame.init()\n",
    "\n",
    "# Load the beep sound\n",
    "beep_sound = pygame.mixer.Sound(\"Beep_cut_2.wav\")\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model_loaded.eval()\n",
    "\n",
    "# Open a video capture object (0 for webcam)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Check if the camera is opened correctly\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open camera.\")\n",
    "    exit()\n",
    "\n",
    "# Define class labels\n",
    "class_labels = ['Awake', 'Drowsy']\n",
    "\n",
    "# Initialize variables for time tracking\n",
    "start_time = None\n",
    "drowsy_detected = False\n",
    "\n",
    "# Read frames from the video capture object\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Preprocess the frame\n",
    "    input_tensor = preprocess(frame).unsqueeze(0)\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        output = model_loaded(input_tensor)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        predicted_label = class_labels[predicted.item()]\n",
    "\n",
    "    # Display the predicted label on the frame\n",
    "    cv2.putText(frame, predicted_label, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Check if the predicted label is \"Drowsy\"\n",
    "    if predicted_label == 'Drowsy':\n",
    "        # If \"Drowsy\" label detected for the first time, start the timer\n",
    "        if not drowsy_detected:\n",
    "            start_time = time.time()\n",
    "            drowsy_detected = True\n",
    "        # If \"Drowsy\" label is detected for 2 seconds continuously, play the beep sound\n",
    "        elif time.time() - start_time >= 2:\n",
    "            beep_sound.play()\n",
    "    else:\n",
    "        # Reset timer and flag if label is not \"Drowsy\"\n",
    "        start_time = None\n",
    "        drowsy_detected = False\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # Delay for 1 second\n",
    "    time.sleep(0.01)\n",
    "\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7dc445",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
